{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 5 \n",
    "## Find the income using Support Vector Machines!\n",
    "\n",
    "\n",
    "From the link to adult.zip download the data set. First, take a look at the data. You can see that the data contains categorical data as well. First of run a random forests and measure your performance.\n",
    "\n",
    "\n",
    "the first part fo the assignment, I am gonig to use part of Abtin's code to create the functions and classes needed to run Random Forest and measure the performance \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import random\n",
    "import numpy as np \n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statistics import mean, stdev\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Entropy(prob_X):\n",
    "    \"\"\"\n",
    "    This is the entropy function that computes \n",
    "    entropy  of a  given  random  variables  X \n",
    "    and with  their corresponding probabilities \n",
    "    p_i based on the definition in:\n",
    "    \n",
    "    Shanon and Weaver, 1949 \n",
    "    \n",
    "    -> Links to paper :\n",
    "    --> http://math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf\n",
    "    --> https://ieeexplore.ieee.org/document/6773024\n",
    "    \n",
    "    \n",
    "              Entropy = Σ_i p_i * log2 (p_i)\n",
    "              \n",
    "              \n",
    "    INPUT:\n",
    "    -------\n",
    "            prob_X (a list/array of variables):  \n",
    "       \n",
    "       it should contains all the  probabilities of \n",
    "       the underlying random variable, each element\n",
    "       expected to be a (0 <= float) and should\n",
    "       add up to 1. (Else will be normalized)\n",
    "       \n",
    "       \n",
    "       \n",
    "    OUTPUT:\n",
    "    -------\n",
    "            Entropy (float): Entropy bits \n",
    "    \"\"\"\n",
    "    import math\n",
    "    _sum_ = 0\n",
    "    \n",
    "    _tot_ = 0\n",
    "    # checks\n",
    "    for prob in prob_X:\n",
    "        assert prob >= 0, \"Negative probability is not accepted!!!\"\n",
    "        _tot_ += prob\n",
    "    \n",
    "#     if _tot_!=1:\n",
    "#         print(\"Inputs are not normalized added up to {}, will be normalized!!\".format(_tot_))\n",
    "    \n",
    "    for prob in prob_X:\n",
    "        if _tot_==0:\n",
    "            continue\n",
    "            \n",
    "        prob = prob/_tot_\n",
    "        if prob == 0:\n",
    "            pass\n",
    "        else:\n",
    "            _sum_ += prob * math.log2(prob)\n",
    "        \n",
    "    return abs(_sum_)\n",
    "\n",
    "\n",
    "def Boolean_Entropy(q):\n",
    "    \"\"\"\n",
    "    Finds the entropy for a Boolean random variable. \n",
    "    \n",
    "    INPUT:\n",
    "    ------\n",
    "           q (float) : is expected to be between 0 and 1 (else AssertionError)\n",
    "           \n",
    "    OUTPUT:\n",
    "    -------\n",
    "            Entropy (float) : Entropy of a throwing a coin with chances \n",
    "                              of P(H, T) = (q, 1 - q) in bits\n",
    "                              \n",
    "                              \n",
    "    \"\"\"\n",
    "    assert q >= 0 and q <= 1, \"q = {} is not between [0,1]!\".format(q)\n",
    "        \n",
    "    return Entropy([q, 1-q])\n",
    "\n",
    "\n",
    "def Boolean_Entropy_counts(p, n):\n",
    "    \"\"\"\n",
    "    Finds the entropy for a Boolean random variable. \n",
    "    \n",
    "    INPUT:\n",
    "    ------\n",
    "           p (int or float) : Number or relative fraction of positive instances\n",
    "           n (int or float) : Number or relative fraction of negative instances\n",
    "           \n",
    "    OUTPUT:\n",
    "    -------\n",
    "            Entropy (float) : Entropy of a throwing a coin with chances \n",
    "                              of P(H, T) = (q, 1 - q) in bits\n",
    "                              \n",
    "                              with q = p / (n + p)\n",
    "                              \n",
    "    \"\"\"\n",
    "    if n==0 and p==0:\n",
    "        return 0\n",
    "    q = p / (n + p)\n",
    "    return Boolean_Entropy(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(X, Y):\n",
    "    return math.sqrt(sum((x - y)**2 for x, y in zip(X, Y)))\n",
    "\n",
    "\n",
    "def cross_entropy_loss(X, Y):\n",
    "    n=len(X)\n",
    "    return (-1.0/n)*sum(x*math.log(y) + (1-x)*math.log(1-y) for x, y in zip(X, Y))\n",
    "\n",
    "\n",
    "def rms_error(X, Y):\n",
    "    return math.sqrt(ms_error(X, Y))\n",
    "\n",
    "\n",
    "def ms_error(X, Y):\n",
    "    return mean((x - y)**2 for x, y in zip(X, Y))\n",
    "\n",
    "\n",
    "def mean_error(X, Y):\n",
    "    return mean(abs(x - y) for x, y in zip(X, Y))\n",
    "\n",
    "\n",
    "def manhattan_distance(X, Y):\n",
    "    return sum(abs(x - y) for x, y in zip(X, Y))\n",
    "\n",
    "\n",
    "def mean_boolean_error(X, Y):\n",
    "    return mean(int(x != y) for x, y in zip(X, Y))\n",
    "\n",
    "\n",
    "def hamming_distance(X, Y):\n",
    "    return sum(x != y for x, y in zip(X, Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_data_set(data_file, skiprows=0, separator=None):\n",
    "    with open(data_file, \"r\") as f:\n",
    "        file = f.read()\n",
    "        lines = file.splitlines()\n",
    "        lines = lines[skiprows:]\n",
    "        \n",
    "    data_ = [[] for _ in range(len(lines))]\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        splitted_line = line.split(separator)\n",
    "        float_line = []\n",
    "        for value in splitted_line:\n",
    "            try:\n",
    "                value = float(value)\n",
    "            except ValueError:\n",
    "                if value==\"\":\n",
    "                    continue\n",
    "                else:\n",
    "                    pass\n",
    "            float_line.append(value)\n",
    "        if float_line:\n",
    "            data_[i] = float_line \n",
    "    \n",
    "    for line in data_:\n",
    "        if not line:\n",
    "            data_.remove(line)\n",
    "    \n",
    "    return data_\n",
    "\n",
    "def unique(seq):\n",
    "    \"\"\"\n",
    "    Remove any duplicate elements from any sequence,\n",
    "    works on hashable elements such as int, float,\n",
    "    string, and tuple.\n",
    "    \"\"\"\n",
    "    return list(set(seq))\n",
    "\n",
    "\n",
    "def remove_all(item, seq):\n",
    "    \"\"\"Return a copy of seq (or string) with all occurrences of item removed.\"\"\"\n",
    "    if isinstance(seq, str):\n",
    "        return seq.replace(item, '')\n",
    "    else:\n",
    "        return [x for x in seq if x != item]\n",
    "    \n",
    "    \n",
    "def weighted_sample_with_replacement(n, seq, weights):\n",
    "    \"\"\"Pick n samples from seq at random, with replacement, with the\n",
    "    probability of each element in proportion to its corresponding\n",
    "    weight.\"\"\"\n",
    "    sample = weighted_sampler(seq, weights)\n",
    "\n",
    "    return [sample() for _ in range(n)]\n",
    "    \n",
    "\n",
    "def weighted_sampler(seq, weights):\n",
    "    \"\"\"Return a random-sample function that picks from seq weighted by weights.\"\"\"\n",
    "    import bisect\n",
    "\n",
    "    totals = []\n",
    "    for w in weights:\n",
    "        totals.append(w + totals[-1] if totals else w)\n",
    "\n",
    "    return lambda: seq[bisect.bisect(totals, random.uniform(0, totals[-1]))]\n",
    "    \n",
    "    \n",
    "def mode(data):\n",
    "    import collections\n",
    "    \"\"\"Return the most common data item. If there are ties, return any one of them.\"\"\"\n",
    "    [(item, count)] = collections.Counter(data).most_common(1)\n",
    "    return item\n",
    "\n",
    "# argmin and argmax\n",
    "\n",
    "identity = lambda x: x\n",
    "\n",
    "argmin = min\n",
    "argmax = max\n",
    "\n",
    "\n",
    "def argmin_random_tie(seq, key=identity):\n",
    "    \"\"\"Return a minimum element of seq; break ties at random.\"\"\"\n",
    "    return argmin(shuffled(seq), key=key)\n",
    "\n",
    "\n",
    "def argmax_random_tie(seq, key=identity):\n",
    "    \"\"\"Return an element with highest fn(seq[i]) score; break ties at random.\"\"\"\n",
    "    return argmax(shuffled(seq), key=key)\n",
    "\n",
    "\n",
    "def shuffled(iterable):\n",
    "    \"\"\"Randomly shuffle a copy of iterable.\"\"\"\n",
    "    items = list(iterable)\n",
    "    random.shuffle(items)\n",
    "    return items\n",
    "\n",
    "def check_equal(iterator):\n",
    "    iterator = iter(iterator)\n",
    "    try:\n",
    "        first = next(iterator)\n",
    "    except StopIteration:\n",
    "        return True\n",
    "    return all(first == rest for rest in iterator)\n",
    "\n",
    "def probability(p):\n",
    "    \"\"\"Return true with probability p.\"\"\"\n",
    "    return p > random.uniform(0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Set:\n",
    "    \"\"\"\n",
    "    Defining a _general_ data set class for machine learning. \n",
    "    \n",
    "    These are the following fields:\n",
    "    \n",
    "    >> data = Data_set\n",
    "\n",
    "    data.examples:\n",
    "           \n",
    "           list of examples. Each one is a list contains of attribute values.\n",
    "\n",
    "    \n",
    "    data.attributes:\n",
    "    \n",
    "           list of integers to index into an example, so example[attribute]\n",
    "           gives a value.\n",
    "    \n",
    "    \n",
    "    data.attribute_names:\n",
    "             \n",
    "           list of names for corresponding attributes.\n",
    "    \n",
    "    \n",
    "    data.target_attribute:\n",
    "    \n",
    "           The target attribute for the learning algorithm.\n",
    "           (Default = last attribute)\n",
    "    \n",
    "    \n",
    "    data.input_attributes:\n",
    "             \n",
    "           The list of attributes without the target.\n",
    "    \n",
    "    \n",
    "    \n",
    "    data.values:\n",
    "             \n",
    "           It is a list of lists in which each sublist is the\n",
    "           set of possible values for the corresponding attribute. \n",
    "           If initially None, it is computed from the known examples\n",
    "           by self.setproblem. If not None, bad value raises ValueError.\n",
    "           \n",
    "           \n",
    "    data.distance_measure:\n",
    "                 \n",
    "           A measure  of  distance  function which takes two examples\n",
    "           and returns a nonnegative number. It should be a symmetric \n",
    "           function. \n",
    "           (Defaults = mean_boolean_error) : can handle any field types.\n",
    "           \n",
    "           \n",
    "    data.file_info:\n",
    "           \n",
    "           This should be a tuple that contains:\n",
    "        (file_address, number of rows to skip, separator)\n",
    "           \n",
    "           \n",
    "           \n",
    "    data.name:\n",
    "            \n",
    "           This is for naming the data set. \n",
    "    \n",
    "    \n",
    "    \n",
    "    data.source:\n",
    "    \n",
    "            URL or explanation to the dataset main source\n",
    "            \n",
    "            \n",
    "    data.excluded_attributes:\n",
    "     \n",
    "            List of attribute indexes to exclude from data.input_attributes.\n",
    "            (indexes or names of the attributes)\n",
    "\n",
    "    Normally, you call the constructor and you're done; then you just\n",
    "    access fields like d.examples and d.target and d.inputs.\"\"\"\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, examples=None, attributes=None,  attribute_names=None, \n",
    "                 target_attribute = -1,  input_attributes=None,  values=None, \n",
    "                 distance_measure = mean_boolean_error, name='',  source='', \n",
    "                 excluded_attributes=(), file_info=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Accepts any of DataSet's fields. Examples can also be a\n",
    "        string or file from which to parse examples using parse_csv.\n",
    "        Optional parameter: exclude, as documented in .setproblem().\n",
    "        \n",
    "        >>> DataSet(examples='1, 2, 3')\n",
    "        <DataSet(): 1 examples, 3 attributes>\n",
    "        \"\"\"\n",
    "        \n",
    "        self.file_info = file_info\n",
    "        self.name = name\n",
    "        self.source = source\n",
    "        self.values = values\n",
    "        self.distance = distance_measure\n",
    "        self.check_values_flag = bool(values)\n",
    "\n",
    "        # Initialize examples from a list\n",
    "        if examples is not None:\n",
    "            self.examples = examples\n",
    "        elif file_info is None:\n",
    "            raise ValueError(\"No Examples! and No Address!\")\n",
    "        else:\n",
    "            self.examples = _read_data_set(file_info[0], file_info[1], file_info[2])\n",
    "\n",
    "        # Attributes are the index of examples. can be overwrite \n",
    "        if self.examples is not None and attributes is None:\n",
    "            attributes = list(range(len(self.examples[0])))\n",
    "\n",
    "        self.attributes = attributes\n",
    "        \n",
    "        # Initialize attribute_names from string, list, or to default\n",
    "        if isinstance(attribute_names, str):\n",
    "            self.attribute_names = attribute_names.split()\n",
    "        else:\n",
    "            self.attribute_names = attribute_names or attributes\n",
    "            \n",
    "        # set the definitions needed for the problem \n",
    "        self.set_problem(target_attribute, input_attributes=input_attributes, \n",
    "                         excluded_attributes=excluded_attributes)\n",
    "\n",
    "        \n",
    "        \n",
    "    def get_attribute_num(self, attribute):\n",
    "        if isinstance(attribute, str):\n",
    "            return self.attribute_names.index(attribute)\n",
    "        else:\n",
    "            return attribute\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def set_problem(self, target_attribute, input_attributes=None, excluded_attributes=()):\n",
    "        \"\"\"\n",
    "        By doing this we set the target, inputs and excluded attributes.\n",
    "        \n",
    "        This way, one DataSet can be used multiple ways. inputs, if specified,\n",
    "        is a list of attributes, or specify exclude as a list of attributes\n",
    "        to not use in inputs. Attributes can be -n .. n, or an attrname.\n",
    "        Also computes the list of possible values, if that wasn't done yet.\"\"\"\n",
    "        \n",
    "        self.target_attribute = self.get_attribute_num(target_attribute)\n",
    "        \n",
    "        exclude = [self.get_attribute_num(excluded) for excluded in excluded_attributes]\n",
    "        \n",
    "        if input_attributes:\n",
    "            self.input_attributes = remove_all(self.target_attribute, input_attributes)\n",
    "        else:\n",
    "            inputs = []\n",
    "            for a in self.attributes:\n",
    "                if a != self.target_attribute and a not in exclude:\n",
    "                    inputs.append(a)\n",
    "            self.input_attributes = inputs\n",
    "\n",
    "        if not self.values:\n",
    "            self.update_values()\n",
    "        self.sanity_check()\n",
    "        \n",
    "        \n",
    "    def sanity_check(self):\n",
    "        \"\"\"Sanity check on the fields.\"\"\"\n",
    "        \n",
    "        assert len(self.attribute_names) == len(self.attributes)\n",
    "        assert self.target_attribute in self.attributes\n",
    "        assert self.target_attribute not in self.input_attributes\n",
    "        assert set(self.input_attributes).issubset(set(self.attributes))\n",
    "        if self.check_values_flag:\n",
    "            # only check if values are provided while initializing DataSet\n",
    "            [self.check_example(example) for example in self.examples]\n",
    "        \n",
    "\n",
    "    def check_example(self, example):\n",
    "        if self.values:\n",
    "            for attr in self.attributes:\n",
    "                if example[attr] not in self.values:\n",
    "                    raise ValueError(\"Not recognized value of {} for attribute {} in {}\"\n",
    "                                     .format(example[attr], attr, example))\n",
    "                    \n",
    "                    \n",
    "    def add_example(self, example):\n",
    "        self.check_example(example)\n",
    "        self.examples.append(example)\n",
    "        \n",
    "    \n",
    "    def update_values(self):\n",
    "        self.values = list(map(unique, zip(*self.examples)))\n",
    "        \n",
    "        \n",
    "    def remove_examples(self, value=\"\"):\n",
    "        self.examples = [example for example in examples if value not in example]\n",
    "\n",
    "    def sanitize(self, example):  \n",
    "        \"\"\"Copy of the examples with non input_attributes replaced by None\"\"\"\n",
    "        _list_ = []\n",
    "        for i, attr_i in enumerate(example):\n",
    "            if i in self.input_attributes:\n",
    "                _list_.append(attr_i)\n",
    "            else:\n",
    "                _list_.append(None)\n",
    "        return _list_\n",
    "    \n",
    "    def train_test_split(self, test_fraction=0.3, Seed = 99):\n",
    "        import numpy as np\n",
    "        \n",
    "        examples = self.examples\n",
    "        atrrs = self.attributes\n",
    "        atrrs_name = self.attribute_names\n",
    "        target = self.target_attribute\n",
    "        input_ = self.input_attributes\n",
    "        name = self.name \n",
    "\n",
    "        np.random.seed(Seed)\n",
    "        _test_index = np.random.choice(list(range(len(examples))), int(test_fraction * len(examples)), replace=False)\n",
    "\n",
    "        test_examples = [example for i, example in enumerate(examples) if i in _test_index]\n",
    "\n",
    "        train_examples = [example for example in examples if example not in test_examples]\n",
    "\n",
    "        Test_data_set = Data_Set(examples=test_examples,\n",
    "                                 attributes=atrrs,\n",
    "                                 attribute_names=attr_names,\n",
    "                                 target_attribute=target,\n",
    "                                 input_attributes=input_,\n",
    "                                 name=name + \" Test set\",)\n",
    "\n",
    "        Train_data_set = Data_Set(examples=train_examples,\n",
    "                                 attributes=atrrs,\n",
    "                                 attribute_names=attr_names,\n",
    "                                 target_attribute=target,\n",
    "                                 input_attributes=input_,\n",
    "                                 name=name + \" Train set\",)\n",
    "\n",
    "        return Train_data_set, Test_data_set\n",
    "                \n",
    "        \n",
    "    def __repr__(self):\n",
    "        return '<DataSet({}): with {} examples, and {} attributes>'.format(\n",
    "            self.name, len(self.examples), len(self.attributes))\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decision_Branch:\n",
    "    \"\"\"\n",
    "    A branch of a decision tree holds an attribute to test, and a dict\n",
    "    of branches for each attribute's values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, attribute, attribute_name=None, default_child=None, branches=None):\n",
    "        \"\"\"Initialize by specifying what attribute this node tests.\"\"\"\n",
    "        \n",
    "        self.attribute = attribute\n",
    "        self.attribute_name = attribute_name or attribute\n",
    "        self.default_child = default_child\n",
    "        self.branches = branches or {}\n",
    "\n",
    "    def __call__(self, example):\n",
    "        \"\"\"Classify a given example using the attribute and the branches.\"\"\"\n",
    "        attribute_val = example[self.attribute]\n",
    "        if attribute_val in self.branches:\n",
    "            return self.branches[attribute_val](example)\n",
    "        else:\n",
    "            # return default class when attribute is unknown\n",
    "            return self.default_child(example)\n",
    "\n",
    "    def add(self, value, subtree):\n",
    "        \"\"\"Add a branch.  If self.attribute = value, move to the given subtree.\"\"\"\n",
    "        self.branches[value] = subtree\n",
    "\n",
    "    def display_out(self, indent=0):\n",
    "        name = self.attribute_name\n",
    "        print(\"Test\", name)\n",
    "        for value, subtree in self.branches.items():\n",
    "            print(\" \" * indent * 5, name, '=', value, \"--->\", end=\" \")\n",
    "            subtree.display_out(indent + 1)\n",
    "        # New line\n",
    "        print()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return ('Decision_Branch({}, {}, {})'\n",
    "                .format(self.attribute, self.attribute_name, self.branches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decision_Leaf:\n",
    "    \"\"\"A simple leaf class for a decision tree that hold a result.\"\"\"\n",
    "\n",
    "    def __init__(self, result):\n",
    "        self.result = result\n",
    "\n",
    "    def __call__(self, example):\n",
    "        return self.result\n",
    "\n",
    "    def display_out(self, indent=0):\n",
    "        print('RESULT =', self.result)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Decision_Tree_Learner(dataset):\n",
    "    \"\"\"\n",
    "    Learning Algorithm for a Decision Tree\n",
    "    \"\"\"\n",
    "\n",
    "    target, values = dataset.target_attribute, dataset.values\n",
    "\n",
    "    def decision_tree_learning(examples, attrs, parent_examples=()):\n",
    "        if not examples:\n",
    "            return plurality(parent_examples)\n",
    "        elif same_class_for_all(examples):\n",
    "            return Decision_Leaf(examples[0][target])\n",
    "        elif not attrs:\n",
    "            return plurality(examples)\n",
    "        else:\n",
    "            A = choose_important_attribute(attrs, examples)\n",
    "            tree = Decision_Branch(A, dataset.attribute_names[A], plurality(examples))\n",
    "            \n",
    "            for (vk, exs) in split_by(A, examples):\n",
    "                subtree = decision_tree_learning(\n",
    "                    exs, remove_all(A, attrs), examples)\n",
    "                tree.add(vk, subtree)\n",
    "            return tree\n",
    "\n",
    "    def plurality(examples):\n",
    "        \"\"\"Return the most occured target value for this set of examples.\n",
    "        (If binary target this is the majority, otherwise plurality)\"\"\"\n",
    "        most_occured = argmax_random_tie(values[target],\n",
    "                                    key=lambda v: count_example_same_attr(target, v, examples))\n",
    "        return Decision_Leaf(most_occured)\n",
    "\n",
    "    def count_example_same_attr(attr, val, examples):\n",
    "        \"\"\"Count the number of examples that have example[attr] = val.\"\"\"\n",
    "        return sum(e[attr] == val for e in examples)\n",
    "\n",
    "    def same_class_for_all(examples):\n",
    "        \"\"\"Are all these examples in the same target class?\"\"\"\n",
    "        _class_ = examples[0][target]\n",
    "        return all(example[target] == _class_ for example in examples)\n",
    "\n",
    "    def choose_important_attribute(attrs, examples):\n",
    "        \"\"\"Choose the attribute with the highest information gain.\"\"\"\n",
    "        return argmax_random_tie(attrs,\n",
    "                                 key=lambda a: information_gain(a, examples))\n",
    "\n",
    "    def information_gain(attr, examples):\n",
    "        \"\"\"Return the expected reduction in entropy from splitting by attr.\"\"\"\n",
    "        def _entropy_(examples):\n",
    "            count = []\n",
    "            for val in values[target]:\n",
    "                count.append(count_example_same_attr(target, val, examples))\n",
    "            return Entropy(count)\n",
    "        \n",
    "        N = len(examples)\n",
    "        remainder = sum((len(examples_i)/N) * _entropy_(examples_i)\n",
    "                        for (v, examples_i) in split_by(attr, examples))\n",
    "        return _entropy_(examples) - remainder\n",
    "\n",
    "    def split_by(attr, examples):\n",
    "        \"\"\"Return a list of (val, examples) pairs for each val of attr.\"\"\"\n",
    "        return [(v, [e for e in examples if e[attr] == v])\n",
    "                for v in values[attr]]\n",
    "\n",
    "    return decision_tree_learning(dataset.examples, dataset.input_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Forest(dataset, n=5, verbose=False):\n",
    "    \"\"\"\n",
    "    An ensemble of Decision Trees trained using bagging and feature bagging.\n",
    "    \n",
    "    bagging: Bootstrap aggregating\n",
    "    \"\"\"\n",
    "\n",
    "    def data_bagging(dataset, m=0):\n",
    "        \"\"\"Sample m examples with replacement\"\"\"\n",
    "        n = len(dataset.examples)\n",
    "        return weighted_sample_with_replacement(m or n, dataset.examples, [1]*n)\n",
    "\n",
    "    def feature_bagging(dataset, p=0.7):\n",
    "        \"\"\"Feature bagging with probability p to retain an attribute\"\"\"\n",
    "        inputs = [i for i in dataset.input_attributes if probability(p)]\n",
    "        return inputs or dataset.input_attributes\n",
    "\n",
    "    def predict(example):\n",
    "        if verbose:\n",
    "            print([predictor(example) for predictor in predictors])\n",
    "        return mode(predictor(example) for predictor in predictors)\n",
    "\n",
    "    predictors = [Decision_Tree_Learner(Data_Set(examples=data_bagging(dataset),\n",
    "                                                 attributes=dataset.attributes,\n",
    "                                                 attribute_names=dataset.attribute_names,\n",
    "                                                 target_attribute=dataset.target_attribute,\n",
    "                                                 input_attributes=feature_bagging(dataset))) for _ in range(n)]\n",
    "\n",
    "    return predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, we are going to import the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_data = \"/Users/User/OneDrive/FoundationsML/Week5/adult/adult.data\" #home\n",
    "#adult_data = \"/Users/scabellos/OneDrive - Urban Science/User Documents/MastersDegree/ML/Week5/adult.data\" #office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_names = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"maritalstatus\", \"occupation\", \"relationship\", \"race\", \"sex\",\"capitalgain\",\"capitalloss\",\"hoursperweek\",\"nativecountry\",\"IncomeYear\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "AD_dataset = Data_Set(name = \"Adult dataset\", \n",
    "                      target_attribute=14,\n",
    "                      attribute_names=attr_names,\n",
    "                      file_info=(adult_data, None, \",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DataSet(Adult dataset): with 32561 examples, and 15 attributes>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AD_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we are going to  : \n",
    "\n",
    "\n",
    "1. Split the data\n",
    "2. Run Random Forest \n",
    "3. Measure the accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "AD_Train, AD_test = AD_dataset.train_test_split(test_fraction=0.2, Seed=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_AD = Random_Forest(AD_Train, n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Measure_accuracy(true_values, predictions):\n",
    "    _sum_ = 0\n",
    "    for truth, prediction in zip(true_values, predictions):\n",
    "        if truth==prediction:\n",
    "            _sum_+=1\n",
    "    return _sum_/len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7811732186732187"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_pridictions = [RF_AD(a[:-1]) for a in AD_test.examples]\n",
    "true_Test_values = [example[AD_test.target_attribute] for example in AD_test.examples]\n",
    "\n",
    "Measure_accuracy(true_Test_values, rf_pridictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy on Random Forest is 78.11 %\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Categorical data into numbers!\n",
    "\n",
    "\n",
    "\n",
    "Now that we have more complicated algorithm, let’s make use out of them. But ﬁrst you should change your categorical data into real valued number which are needed for the SVM algorithm. Come up with a method that can do this translation\n",
    "\n",
    "\n",
    "\n",
    "First, I am gonig to convert the dataset into a pandas dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'workclass',\n",
       " 'fnlwgt',\n",
       " 'education',\n",
       " 'education-num',\n",
       " 'maritalstatus',\n",
       " 'occupation',\n",
       " 'relationship',\n",
       " 'race',\n",
       " 'sex',\n",
       " 'capitalgain',\n",
       " 'capitalloss',\n",
       " 'hoursperweek',\n",
       " 'nativecountry',\n",
       " 'IncomeYear']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AD_dataset.attribute_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "    #Convert list of tuples to dataframe and set column names and indexes\n",
    "AD_pd = pd.DataFrame(data=AD_dataset.examples,columns=AD_dataset.attribute_names,index=list(range(0,32561))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561, 15)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AD_pd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the data, we are going to look for duplicates, missing values ... etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 32561 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      "age              32561 non-null float64\n",
      "workclass        32561 non-null object\n",
      "fnlwgt           32561 non-null float64\n",
      "education        32561 non-null object\n",
      "education-num    32561 non-null float64\n",
      "maritalstatus    32561 non-null object\n",
      "occupation       32561 non-null object\n",
      "relationship     32561 non-null object\n",
      "race             32561 non-null object\n",
      "sex              32561 non-null object\n",
      "capitalgain      32561 non-null float64\n",
      "capitalloss      32561 non-null float64\n",
      "hoursperweek     32561 non-null float64\n",
      "nativecountry    32561 non-null object\n",
      "IncomeYear       32561 non-null object\n",
      "dtypes: float64(6), object(9)\n",
      "memory usage: 4.0+ MB\n"
     ]
    }
   ],
   "source": [
    "AD_pd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for duplicates \n",
    "AD_pd=AD_pd.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop all \"?\"= missing values in the data \n",
    "\n",
    "AD_pd=AD_pd.drop(AD_pd[AD_pd['workclass']==' ?'].index)\n",
    "AD_pd=AD_pd.drop(AD_pd[AD_pd['occupation']==' ?'].index)\n",
    "AD_pd=AD_pd.drop(AD_pd[AD_pd['nativecountry']==' ?'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>maritalstatus</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capitalgain</th>\n",
       "      <th>capitalloss</th>\n",
       "      <th>hoursperweek</th>\n",
       "      <th>nativecountry</th>\n",
       "      <th>IncomeYear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [age, workclass, fnlwgt, education, education-num, maritalstatus, occupation, relationship, race, sex, capitalgain, capitalloss, hoursperweek, nativecountry, IncomeYear]\n",
       "Index: []"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AD_pd[AD_pd.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30139, 15)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AD_pd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many unique values we have in the data set ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                 72\n",
       "workclass            7\n",
       "fnlwgt           20263\n",
       "education           16\n",
       "education-num       16\n",
       "maritalstatus        7\n",
       "occupation          14\n",
       "relationship         6\n",
       "race                 5\n",
       "sex                  2\n",
       "capitalgain        118\n",
       "capitalloss         90\n",
       "hoursperweek        94\n",
       "nativecountry       41\n",
       "IncomeYear           2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AD_pd.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### All these are categorical values - before we run SVM, we need to transform them into numbers. Lets analyze the data first to see if we can do some groupings and then use hot encoder to convert all categorical attributes \n",
    "\n",
    "#### workclass  --  categorical  ( 7 distinct values) \n",
    "#### education   --    categorical  ( 16 distinct values) \n",
    "#### maritalstatus --    categorical ( 7 distinct values) \n",
    "#### occupation   --    categorical ( 14 distinct values) \n",
    "#### relationship  --   categorical ( 6 distinct values) \n",
    "#### race    ---         categorical ( 5 distinct values) \n",
    "#### sex     ---        categorical ( 2 values )\n",
    "#### nativecountry ---   categorical  ( 41 distinct values)  \n",
    "#### IncomeYear  ---     categorical ( 2 values  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a copy just in case i need to test things \n",
    "AD_pd_copy=AD_pd.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For IncomeYear (our target) we will use the cat encoder - \n",
    "\n",
    "#### I will categorize into 2 values. '>50K' = 1 and '<=50K' = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " <=50K    22633\n",
       " >50K      7506\n",
       "Name: IncomeYear, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AD_pd[\"IncomeYear\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoricalDtype(categories=[' <=50K', ' >50K'], ordered=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AD_pd[\"IncomeYear_cat\"] = AD_pd[\"IncomeYear\"].astype('category')\n",
    "AD_pd[\"IncomeYear_cat\"].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>maritalstatus</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capitalgain</th>\n",
       "      <th>capitalloss</th>\n",
       "      <th>hoursperweek</th>\n",
       "      <th>nativecountry</th>\n",
       "      <th>IncomeYear</th>\n",
       "      <th>IncomeYear_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.0</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516.0</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50.0</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311.0</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646.0</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721.0</td>\n",
       "      <td>11th</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409.0</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age          workclass    fnlwgt   education  education-num  \\\n",
       "0  39.0          State-gov   77516.0   Bachelors           13.0   \n",
       "1  50.0   Self-emp-not-inc   83311.0   Bachelors           13.0   \n",
       "2  38.0            Private  215646.0     HS-grad            9.0   \n",
       "3  53.0            Private  234721.0        11th            7.0   \n",
       "4  28.0            Private  338409.0   Bachelors           13.0   \n",
       "\n",
       "         maritalstatus          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capitalgain  capitalloss  hoursperweek   nativecountry IncomeYear  \\\n",
       "0       2174.0          0.0          40.0   United-States      <=50K   \n",
       "1          0.0          0.0          13.0   United-States      <=50K   \n",
       "2          0.0          0.0          40.0   United-States      <=50K   \n",
       "3          0.0          0.0          40.0   United-States      <=50K   \n",
       "4          0.0          0.0          40.0            Cuba      <=50K   \n",
       "\n",
       "   IncomeYear_cat  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AD_pd[\"IncomeYear_cat\"] = AD_pd[\"IncomeYear_cat\"].cat.codes\n",
    "AD_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    22633\n",
       "1     7506\n",
       "Name: IncomeYear_cat, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AD_pd[\"IncomeYear_cat\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st attribute I need to look into -  Sex -  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " Male      20366\n",
       " Female     9773\n",
       "Name: sex, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AD_pd[\"sex\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we are going to analyze \"Race\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " White                 25912\n",
       " Black                  2816\n",
       " Asian-Pac-Islander      894\n",
       " Amer-Indian-Eskimo      286\n",
       " Other                   231\n",
       "Name: race, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AD_pd[\"race\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at the 2 last categories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 16)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "race_analysis = AD_pd[AD_pd['race'].str.contains('Amer-Indian-Eskimo')& AD_pd['IncomeYear'].str.contains('>50K')]\n",
    "race_analysis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 16)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_analysis = AD_pd[AD_pd['race'].str.contains('Other')& AD_pd['IncomeYear'].str.contains('>50K')]\n",
    "race_analysis.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 55 out of the 7506 adults who make more than $50k  are of race Amer-Indian-Eskimo or Other. \n",
    "I am going to merge these 2 categories to make 1 and reduce the number of categories i need to encode. \n",
    "\n",
    "\n",
    "I'll group them all into other and review for duplicates again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "AD_pd[\"race\"]= AD_pd[\"race\"].replace(' Amer-Indian-Eskimo', ' Other') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " White                 25912\n",
       " Black                  2816\n",
       " Asian-Pac-Islander      894\n",
       " Other                   517\n",
       "Name: race, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AD_pd[\"race\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for duplicates \n",
    "AD_pd=AD_pd.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marital Status now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " Married-civ-spouse       14059\n",
       " Never-married             9711\n",
       " Divorced                  4212\n",
       " Separated                  939\n",
       " Widowed                    827\n",
       " Married-spouse-absent      370\n",
       " Married-AF-spouse           21\n",
       "Name: maritalstatus, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AD_pd[\"maritalstatus\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the context of providing Income to a household, I am gonig to group those individuals who do not have a spouse as 1Member_Family and the remaining as 2Member_Family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "AD_pd[\"maritalstatus\"]= AD_pd[\"maritalstatus\"].replace(' Never-married', '1M_Family')\n",
    "AD_pd[\"maritalstatus\"]= AD_pd[\"maritalstatus\"].replace(' Divorced', '1M_Family')\n",
    "AD_pd[\"maritalstatus\"]= AD_pd[\"maritalstatus\"].replace(' Separated', '1M_Family')\n",
    "AD_pd[\"maritalstatus\"]= AD_pd[\"maritalstatus\"].replace(' Married-spouse-absent', '1M_Family')\n",
    "AD_pd[\"maritalstatus\"]= AD_pd[\"maritalstatus\"].replace(' Widowed', '1M_Family')\n",
    "\n",
    "\n",
    "AD_pd[\"maritalstatus\"]= AD_pd[\"maritalstatus\"].replace(' Married-civ-spouse', '2M_Family')\n",
    "AD_pd[\"maritalstatus\"]= AD_pd[\"maritalstatus\"].replace(' Married-AF-spouse', '2M_Family')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1M_Family    16059\n",
       "2M_Family    14080\n",
       "Name: maritalstatus, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AD_pd[\"maritalstatus\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Lets analyze Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " Husband           12457\n",
       " Not-in-family      7714\n",
       " Own-child          4462\n",
       " Unmarried          3211\n",
       " Wife               1406\n",
       " Other-relative      889\n",
       "Name: relationship, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AD_pd[\"relationship\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " Prof-specialty       4034\n",
       " Craft-repair         4025\n",
       " Exec-managerial      3991\n",
       " Adm-clerical         3719\n",
       " Sales                3584\n",
       " Other-service        3209\n",
       " Machine-op-inspct    1964\n",
       " Transport-moving     1572\n",
       " Handlers-cleaners    1349\n",
       " Farming-fishing       987\n",
       " Tech-support          911\n",
       " Protective-serv       644\n",
       " Priv-house-serv       141\n",
       " Armed-Forces            9\n",
       "Name: occupation, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AD_pd[\"occupation\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " Private             22264\n",
       " Self-emp-not-inc     2498\n",
       " Local-gov            2067\n",
       " State-gov            1279\n",
       " Self-emp-inc         1074\n",
       " Federal-gov           943\n",
       " Without-pay            14\n",
       "Name: workclass, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AD_pd[\"workclass\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 16)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workclass_analysis = AD_pd[AD_pd['workclass'].str.contains('Without-pay')& AD_pd['IncomeYear'].str.contains('>50K')]\n",
    "workclass_analysis.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets Remove those rows that dont report PAY as they dont add new information to the final target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AD_pd_copy=AD_pd_copy.drop(['workclass'==' Without-pay'], axis=1)\n",
    "\n",
    "indexNames = AD_pd[AD_pd['workclass'].str.contains('Without-pay')].index\n",
    "AD_pd.drop(indexNames , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " Private             22264\n",
       " Self-emp-not-inc     2498\n",
       " Local-gov            2067\n",
       " State-gov            1279\n",
       " Self-emp-inc         1074\n",
       " Federal-gov           943\n",
       "Name: workclass, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AD_pd[\"workclass\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now i am gonig ot create 3 bucket before onehotcoding -\n",
    "#### Gov Employees / Private Sector / Self Employee "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "AD_pd[\"workclass\"]= AD_pd[\"workclass\"].replace(' Federal-gov', 'gov')\n",
    "AD_pd[\"workclass\"]= AD_pd[\"workclass\"].replace(' State-gov', 'gov')\n",
    "AD_pd[\"workclass\"]= AD_pd[\"workclass\"].replace(' Local-gov', 'gov')\n",
    "AD_pd[\"workclass\"]= AD_pd[\"workclass\"].replace(' Self-emp-not-inc', 'self')\n",
    "AD_pd[\"workclass\"]= AD_pd[\"workclass\"].replace(' Self-emp-inc', 'self')\n",
    "AD_pd[\"workclass\"]= AD_pd[\"workclass\"].replace(' Private', 'priv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "priv    22264\n",
       "gov      4289\n",
       "self     3572\n",
       "Name: workclass, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AD_pd[\"workclass\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Education \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " HS-grad         9825\n",
       " Some-college    6666\n",
       " Bachelors       5042\n",
       " Masters         1626\n",
       " Assoc-voc       1307\n",
       " 11th            1048\n",
       " Assoc-acdm      1007\n",
       " 10th             820\n",
       " 7th-8th          555\n",
       " Prof-school      542\n",
       " 9th              455\n",
       " 12th             377\n",
       " Doctorate        375\n",
       " 5th-6th          287\n",
       " 1st-4th          149\n",
       " Preschool         44\n",
       "Name: education, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AD_pd[\"education\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### is equivalent to education number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0     9825\n",
       "10.0    6666\n",
       "13.0    5042\n",
       "14.0    1626\n",
       "11.0    1307\n",
       "7.0     1048\n",
       "12.0    1007\n",
       "6.0      820\n",
       "4.0      555\n",
       "15.0     542\n",
       "5.0      455\n",
       "8.0      377\n",
       "16.0     375\n",
       "3.0      287\n",
       "2.0      149\n",
       "1.0       44\n",
       "Name: education-num, dtype: int64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AD_pd[\"education-num\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So we are not gonig to consider education ( categorical ) in our model \n",
    "we will drop it from the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we look into Native Country "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " United-States                 27474\n",
       " Mexico                          606\n",
       " Philippines                     187\n",
       " Germany                         128\n",
       " Puerto-Rico                     109\n",
       " Canada                          107\n",
       " India                           100\n",
       " El-Salvador                     100\n",
       " Cuba                             92\n",
       " England                          86\n",
       " Jamaica                          80\n",
       " South                            71\n",
       " China                            68\n",
       " Italy                            68\n",
       " Dominican-Republic               67\n",
       " Vietnam                          64\n",
       " Guatemala                        61\n",
       " Japan                            59\n",
       " Poland                           56\n",
       " Columbia                         56\n",
       " Iran                             42\n",
       " Taiwan                           42\n",
       " Haiti                            42\n",
       " Portugal                         34\n",
       " Nicaragua                        33\n",
       " Peru                             30\n",
       " Greece                           29\n",
       " Ecuador                          27\n",
       " France                           27\n",
       " Ireland                          24\n",
       " Hong                             19\n",
       " Trinadad&Tobago                  18\n",
       " Cambodia                         18\n",
       " Thailand                         17\n",
       " Laos                             17\n",
       " Yugoslavia                       16\n",
       " Outlying-US(Guam-USVI-etc)       14\n",
       " Hungary                          13\n",
       " Honduras                         12\n",
       " Scotland                         11\n",
       " Holand-Netherlands                1\n",
       "Name: nativecountry, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AD_pd[\"nativecountry\"].value_counts()                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets group our native countries into continents. I will leave the USA alone since income from the USA is the most prominent class. \n",
    "\n",
    "#### I am also deleting the values for South since it does not represent a country "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' United-States'  ,'  USA'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Mexico'  ,'  Am'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Philippines'  ,'  Asia'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Germany'  ,'  Eur'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Puerto-Rico'  ,'  Am'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Canada'  ,'  Am'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' El-Salvador'  ,'  Am'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' India'  ,'  Asia'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Cuba'  ,'  Am'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' England'  ,'  Eur'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Jamaica'  ,'  Am'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Italy'  ,'  Eur'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' China'  ,'  Asia'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Dominican-Republic'  ,'  Am'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Vietnam'  ,'  Asia'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Guatemala'  ,'  Am'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Japan'  ,'  Asia'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Poland'  ,'  Eur'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Columbia'  ,'  Am'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Taiwan'  ,'  Asia'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Haiti'  ,'  Am'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Iran'  ,'  Asia'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Portugal'  ,'  Eur'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Nicaragua'  ,'  Am'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Peru'  ,'  Am'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Greece'  ,'  Eur'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' France'  ,'  Eur'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Ecuador'  ,'  Am'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Ireland'  ,'  Eur'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Hong'  ,'  Asia'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Cambodia'  ,'  Asia'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Trinadad&Tobago'  ,'  Am'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Thailand'  ,'  Asia'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Laos'  ,'  Asia'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Yugoslavia'  ,'  Eur'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Outlying-US(Guam-USVI-etc)'  ,'  Am'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Hungary'  ,'  Eur'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Honduras'  ,'  Am'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Scotland'  ,'  Eur'  )\n",
    "AD_pd[\"nativecountry\"]=AD_pd[\"nativecountry\"].replace(' Holand-Netherlands'  ,'  Eur'  )\n",
    "\n",
    "\n",
    "\n",
    "AD_pd=AD_pd.drop(AD_pd[AD_pd['nativecountry']==' South'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  USA     27474\n",
       "  Am       1454\n",
       "  Asia      633\n",
       "  Eur       493\n",
       "Name: nativecountry, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AD_pd[\"nativecountry\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 30054 entries, 0 to 32560\n",
      "Data columns (total 16 columns):\n",
      "age               30054 non-null float64\n",
      "workclass         30054 non-null object\n",
      "fnlwgt            30054 non-null float64\n",
      "education         30054 non-null object\n",
      "education-num     30054 non-null float64\n",
      "maritalstatus     30054 non-null object\n",
      "occupation        30054 non-null object\n",
      "relationship      30054 non-null object\n",
      "race              30054 non-null object\n",
      "sex               30054 non-null object\n",
      "capitalgain       30054 non-null float64\n",
      "capitalloss       30054 non-null float64\n",
      "hoursperweek      30054 non-null float64\n",
      "nativecountry     30054 non-null object\n",
      "IncomeYear        30054 non-null object\n",
      "IncomeYear_cat    30054 non-null int8\n",
      "dtypes: float64(6), int8(1), object(9)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "AD_pd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets work on the encoding  - I am going to use One Hot Encoding for \n",
    "\n",
    "#### Sex, Race ( after grouping ) , MaritalStatus ( after grouping ) , WorkClass , Occupation, Relationship, NativeCountry ( after grouping )\n",
    "\n",
    "The strategy is to convert each category value into a new column and assigns a 1 or 0 (True/False) value to the column. This has the benefit of not weighting a value improperly but does have the downside of adding more columns to the data set.\n",
    "\n",
    "We dont have so many features so it is going to work fine in our data set. Still I am going to keep analyzing the columns to identify other groups to help our module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AD_pd=pd.get_dummies(AD_pd, columns=[\"sex\"])\n",
    "AD_pd=pd.get_dummies(AD_pd, columns=[\"race\"])\n",
    "AD_pd=pd.get_dummies(AD_pd, columns=[\"maritalstatus\"])\n",
    "AD_pd=pd.get_dummies(AD_pd, columns=[\"workclass\"])\n",
    "AD_pd=pd.get_dummies(AD_pd, columns=[\"occupation\"])\n",
    "AD_pd=pd.get_dummies(AD_pd, columns=[\"relationship\"])\n",
    "AD_pd=pd.get_dummies(AD_pd, columns=[\"nativecountry\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capitalgain</th>\n",
       "      <th>capitalloss</th>\n",
       "      <th>hoursperweek</th>\n",
       "      <th>IncomeYear</th>\n",
       "      <th>IncomeYear_cat</th>\n",
       "      <th>sex_ Female</th>\n",
       "      <th>...</th>\n",
       "      <th>relationship_ Husband</th>\n",
       "      <th>relationship_ Not-in-family</th>\n",
       "      <th>relationship_ Other-relative</th>\n",
       "      <th>relationship_ Own-child</th>\n",
       "      <th>relationship_ Unmarried</th>\n",
       "      <th>relationship_ Wife</th>\n",
       "      <th>nativecountry_  Am</th>\n",
       "      <th>nativecountry_  Asia</th>\n",
       "      <th>nativecountry_  Eur</th>\n",
       "      <th>nativecountry_  USA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.0</td>\n",
       "      <td>77516.0</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50.0</td>\n",
       "      <td>83311.0</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38.0</td>\n",
       "      <td>215646.0</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.0</td>\n",
       "      <td>234721.0</td>\n",
       "      <td>11th</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.0</td>\n",
       "      <td>338409.0</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    age    fnlwgt   education  education-num  capitalgain  capitalloss  \\\n",
       "0  39.0   77516.0   Bachelors           13.0       2174.0          0.0   \n",
       "1  50.0   83311.0   Bachelors           13.0          0.0          0.0   \n",
       "2  38.0  215646.0     HS-grad            9.0          0.0          0.0   \n",
       "3  53.0  234721.0        11th            7.0          0.0          0.0   \n",
       "4  28.0  338409.0   Bachelors           13.0          0.0          0.0   \n",
       "\n",
       "   hoursperweek IncomeYear  IncomeYear_cat  sex_ Female         ...           \\\n",
       "0          40.0      <=50K               0            0         ...            \n",
       "1          13.0      <=50K               0            0         ...            \n",
       "2          40.0      <=50K               0            0         ...            \n",
       "3          40.0      <=50K               0            0         ...            \n",
       "4          40.0      <=50K               0            1         ...            \n",
       "\n",
       "   relationship_ Husband  relationship_ Not-in-family  \\\n",
       "0                      0                            1   \n",
       "1                      1                            0   \n",
       "2                      0                            1   \n",
       "3                      1                            0   \n",
       "4                      0                            0   \n",
       "\n",
       "   relationship_ Other-relative  relationship_ Own-child  \\\n",
       "0                             0                        0   \n",
       "1                             0                        0   \n",
       "2                             0                        0   \n",
       "3                             0                        0   \n",
       "4                             0                        0   \n",
       "\n",
       "   relationship_ Unmarried  relationship_ Wife  nativecountry_  Am  \\\n",
       "0                        0                   0                   0   \n",
       "1                        0                   0                   0   \n",
       "2                        0                   0                   0   \n",
       "3                        0                   0                   0   \n",
       "4                        0                   1                   1   \n",
       "\n",
       "   nativecountry_  Asia  nativecountry_  Eur  nativecountry_  USA  \n",
       "0                     0                    0                    1  \n",
       "1                     0                    0                    1  \n",
       "2                     0                    0                    1  \n",
       "3                     0                    0                    1  \n",
       "4                     0                    0                    0  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AD_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, I am going to create a copy, drop the columns we are not using and implement our SVM model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "AD_pd_final=AD_pd.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "AD_pd_final=AD_pd_final.drop(columns=['education','IncomeYear'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capitalgain</th>\n",
       "      <th>capitalloss</th>\n",
       "      <th>hoursperweek</th>\n",
       "      <th>IncomeYear_cat</th>\n",
       "      <th>sex_ Female</th>\n",
       "      <th>sex_ Male</th>\n",
       "      <th>race_ Asian-Pac-Islander</th>\n",
       "      <th>...</th>\n",
       "      <th>relationship_ Husband</th>\n",
       "      <th>relationship_ Not-in-family</th>\n",
       "      <th>relationship_ Other-relative</th>\n",
       "      <th>relationship_ Own-child</th>\n",
       "      <th>relationship_ Unmarried</th>\n",
       "      <th>relationship_ Wife</th>\n",
       "      <th>nativecountry_  Am</th>\n",
       "      <th>nativecountry_  Asia</th>\n",
       "      <th>nativecountry_  Eur</th>\n",
       "      <th>nativecountry_  USA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.0</td>\n",
       "      <td>77516.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50.0</td>\n",
       "      <td>83311.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38.0</td>\n",
       "      <td>215646.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.0</td>\n",
       "      <td>234721.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.0</td>\n",
       "      <td>338409.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    age    fnlwgt  education-num  capitalgain  capitalloss  hoursperweek  \\\n",
       "0  39.0   77516.0           13.0       2174.0          0.0          40.0   \n",
       "1  50.0   83311.0           13.0          0.0          0.0          13.0   \n",
       "2  38.0  215646.0            9.0          0.0          0.0          40.0   \n",
       "3  53.0  234721.0            7.0          0.0          0.0          40.0   \n",
       "4  28.0  338409.0           13.0          0.0          0.0          40.0   \n",
       "\n",
       "   IncomeYear_cat  sex_ Female  sex_ Male  race_ Asian-Pac-Islander  \\\n",
       "0               0            0          1                         0   \n",
       "1               0            0          1                         0   \n",
       "2               0            0          1                         0   \n",
       "3               0            0          1                         0   \n",
       "4               0            1          0                         0   \n",
       "\n",
       "          ...           relationship_ Husband  relationship_ Not-in-family  \\\n",
       "0         ...                               0                            1   \n",
       "1         ...                               1                            0   \n",
       "2         ...                               0                            1   \n",
       "3         ...                               1                            0   \n",
       "4         ...                               0                            0   \n",
       "\n",
       "   relationship_ Other-relative  relationship_ Own-child  \\\n",
       "0                             0                        0   \n",
       "1                             0                        0   \n",
       "2                             0                        0   \n",
       "3                             0                        0   \n",
       "4                             0                        0   \n",
       "\n",
       "   relationship_ Unmarried  relationship_ Wife  nativecountry_  Am  \\\n",
       "0                        0                   0                   0   \n",
       "1                        0                   0                   0   \n",
       "2                        0                   0                   0   \n",
       "3                        0                   0                   0   \n",
       "4                        0                   1                   1   \n",
       "\n",
       "   nativecountry_  Asia  nativecountry_  Eur  nativecountry_  USA  \n",
       "0                     0                    0                    1  \n",
       "1                     0                    0                    1  \n",
       "2                     0                    0                    1  \n",
       "3                     0                    0                    1  \n",
       "4                     0                    0                    0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AD_pd_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=AD_pd_final.drop(columns=['IncomeYear_cat'])\n",
    "y=AD_pd_final['IncomeYear_cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,random_state=109) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the that using the Standard Scaler. Standardize features by removing the mean and scaling to unit variance \n",
    "\n",
    "\n",
    "Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "\n",
    "X_train_sc=scaler.fit_transform(X_train)\n",
    "X_test_sc=scaler.fit_transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we are going to implement our SVM models , running the data through different kernels ( linear, polinomial, sigmoid .... ) and measure the accuracy for those "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kernel</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>linear</td>\n",
       "      <td>0.845958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rbf</td>\n",
       "      <td>0.850394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>poly</td>\n",
       "      <td>0.839969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.781080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Kernel  Accuracy\n",
       "0   linear  0.845958\n",
       "1      rbf  0.850394\n",
       "2     poly  0.839969\n",
       "3  sigmoid  0.781080"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "acc=[]\n",
    "d={}\n",
    "kernels = [\"linear\", \"rbf\", \"poly\", \"sigmoid\"] \n",
    "\n",
    "#for model in range(len(models)):\n",
    "for i in kernels:\n",
    "        clf=SVC(kernel=i)\n",
    "        clf.fit(X_train_sc,y_train)\n",
    "        y_pred=clf.predict(X_test_sc)\n",
    "        acc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "d={'Kernel':kernels,'Accuracy':acc}\n",
    "acc_frame=pd.DataFrame(d)\n",
    "acc_frame\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we can see, rbf ( Radial Base function ) is the Kernel that provides the highest level of accuracy to Support Vector Machine algortihm.\n",
    "\n",
    "#### Just to summarize, these are the steps i took to complete this assignment. \n",
    "\n",
    "\n",
    "#### 1. I imported all the data and run Random Forest and evaluated accuracy. \n",
    "\n",
    "#### 2. Next, I converted the data set into a dataframe and analyzed and grouped some of the categorical variables before using the OneHot encoding method to convert them into numbers. \n",
    "\n",
    "#### 3. I scaled the data appropiately using the Scaler package to standardize our features \n",
    "\n",
    "#### 4. And finally, I implemented a loop to run several SVM implementations and print the accuracy, as we can see, SVM Provides higher accuracy compared to Random Forest. \n",
    "\n",
    "\n",
    "##### Thank you, \n",
    "##### Silvia \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
